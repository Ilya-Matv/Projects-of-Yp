{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is necessary to train the model to classify comments into positive and negative. At your disposal is a dataset with markup on the toxicity of edits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I install the tensorflow libraries and the necessary components of the required version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: tensorflow\n",
      "Version: 2.1.0\n",
      "Summary: TensorFlow is an open source machine learning framework for everyone.\n",
      "Home-page: https://www.tensorflow.org/\n",
      "Author: Google Inc.\n",
      "Author-email: packages@tensorflow.org\n",
      "License: Apache 2.0\n",
      "Location: /opt/conda/lib/python3.7/site-packages\n",
      "Requires: grpcio, termcolor, wheel, keras-preprocessing, opt-einsum, scipy, google-pasta, protobuf, six, tensorboard, wrapt, gast, numpy, absl-py, astor, keras-applications, tensorflow-estimator\n",
      "Required-by: \n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip show tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -q --no-deps tensorflow-addons~=0.8.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Usage:   \n",
      "  /opt/conda/bin/python -m pip uninstall [options] <package> ...\n",
      "  /opt/conda/bin/python -m pip uninstall [options] -r <requirements file> ...\n",
      "\n",
      "no such option: -e\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip uninstall tensorflow protobuf -yes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting typeguard\n",
      "  Downloading typeguard-2.10.0-py3-none-any.whl (16 kB)\n",
      "Installing collected packages: typeguard\n",
      "Successfully installed typeguard-2.10.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install typeguard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import spacy\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, MaxPooling1D, Conv1D, GlobalMaxPooling1D, Dropout, LSTM, GRU\n",
    "from sklearn.metrics import f1_score\n",
    "from tensorflow import argmax as tf_argmax\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from tensorflow.keras import utils\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_addons as tfa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>159566</td>\n",
       "      <td>\":::::And for the second time of asking, when ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>159567</td>\n",
       "      <td>You should be ashamed of yourself \\n\\nThat is ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>159568</td>\n",
       "      <td>Spitzer \\n\\nUmm, theres no actual article for ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>159569</td>\n",
       "      <td>And it looks like it was actually you who put ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>159570</td>\n",
       "      <td>\"\\nAnd ... I really don't think you understand...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>159571 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text  toxic\n",
       "0       Explanation\\nWhy the edits made under my usern...      0\n",
       "1       D'aww! He matches this background colour I'm s...      0\n",
       "2       Hey man, I'm really not trying to edit war. It...      0\n",
       "3       \"\\nMore\\nI can't make any real suggestions on ...      0\n",
       "4       You, sir, are my hero. Any chance you remember...      0\n",
       "...                                                   ...    ...\n",
       "159566  \":::::And for the second time of asking, when ...      0\n",
       "159567  You should be ashamed of yourself \\n\\nThat is ...      0\n",
       "159568  Spitzer \\n\\nUmm, theres no actual article for ...      0\n",
       "159569  And it looks like it was actually you who put ...      0\n",
       "159570  \"\\nAnd ... I really don't think you understand...      0\n",
       "\n",
       "[159571 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('/datasets/toxic_comments.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 159571 entries, 0 to 159570\n",
      "Data columns (total 2 columns):\n",
      "text     159571 non-null object\n",
      "toxic    159571 non-null int64\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 2.4+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data read normally, there are no gaps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Процент класса \"1\": 10.167887648758233\n"
     ]
    }
   ],
   "source": [
    "print('Percent class \"1\":', len(data.loc[data['toxic']==1])/len(data)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is an imbalance in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[\"explanation\\nwhy the edits made under my username hardcore metallica fan were reverted? they weren't vandalisms, just closure on some gas after i voted at new york dolls fac. and please don't remove the template from the talk page since i'm retired now.89.205.38.27\",\n",
       "        0],\n",
       "       [\"d'aww! he matches this background colour i'm seemingly stuck with. thanks.  (talk) 21:51, january 11, 2016 (utc)\",\n",
       "        0],\n",
       "       [\"hey man, i'm really not trying to edit war. it's just that this guy is constantly removing relevant information and talking to me through edits instead of my talk page. he seems to care more about the formatting than the actual info.\",\n",
       "        0],\n",
       "       ['\"\\nmore\\ni can\\'t make any real suggestions on improvement - i wondered if the section statistics should be later on, or a subsection of \"\"types of accidents\"\"  -i think the references may need tidying so that they are all in the exact same format ie date format etc. i can do that later on, if no-one else does first - if you have any preferences for formatting style on references or want to do it yourself please let me know.\\n\\nthere appears to be a backlog on articles for review so i guess there may be a delay until a reviewer turns up. it\\'s listed in the relevant form eg wikipedia:good_article_nominations#transport  \"',\n",
       "        0],\n",
       "       [\"you, sir, are my hero. any chance you remember what page that's on?\",\n",
       "        0],\n",
       "       ['\"\\n\\ncongratulations from me as well, use the tools well. \\xa0· talk \"',\n",
       "        0],\n",
       "       ['cocksucker before you piss around on my work', 1],\n",
       "       [\"your vandalism to the matt shirvington article has been reverted.  please don't do it again, or you will be banned.\",\n",
       "        0],\n",
       "       [\"sorry if the word 'nonsense' was offensive to you. anyway, i'm not intending to write anything in the article(wow they would jump on me for vandalism), i'm merely requesting that it be more encyclopedic so one can use it for school as a reference. i have been to the selective breeding page but it's almost a stub. it points to 'animal breeding' which is a short messy article that gives you no info. there must be someone around with expertise in eugenics? 93.161.107.169\",\n",
       "        0],\n",
       "       ['alignment on this subject and which are contrary to those of dulithgow',\n",
       "        0]], dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['text'] = data['text'].str.lower()#convert words to lower case\n",
    "data.head(10).values#see how the algorithm worked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d aww! he matches this background colour i m seemingly stuck with thanks talk january utc'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#I check how the algorithm for filtering unnecessary characters will work\n",
    "\" \".join(re.sub(r'[^a-z!? ]', ' ', \"d'aww! he matches this background colour i'm seemingly stuck with. thanks.  (talk) 21:51, january 11, 2016 (utc)\").split()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(text):#function for lemmatization of lines of texts\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    word_list = nltk.word_tokenize(text)\n",
    "    lemmatized_output = ' '.join([lemmatizer.lemmatize(w) for w in word_list])\n",
    "    return lemmatized_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"explanation why the edits made under my username hardcore metallica fan were reverted ? they were n't vandalism , just closure on some gas after i voted at new york doll fac . and please do n't remove the template from the talk page since i 'm retired now.89.205.38.27\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatize(data.loc[0,'text'])#check how the function works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"explanation\\nwhy the edits made under my username hardcore metallica fan were reverted? they weren't vandalisms, just closure on some gas after i voted at new york dolls fac. and please don't remove the template from the talk page since i'm retired now.89.205.38.27\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.loc[0,'text']#compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_date = []# pushing lemmatized strings onto the stack\n",
    "for i in data['text'].values:\n",
    "    d = lemmatize(i)\n",
    "    new_date.append(d)\n",
    "df = pd.DataFrame(new_date, columns = ['text'])#save data to dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[\"explanation why the edits made under my username hardcore metallica fan were reverted ? they were n't vandalism , just closure on some gas after i voted at new york doll fac . and please do n't remove the template from the talk page since i 'm retired now.89.205.38.27\"],\n",
       "       [\"d'aww ! he match this background colour i 'm seemingly stuck with . thanks . ( talk ) 21:51 , january 11 , 2016 ( utc )\"],\n",
       "       [\"hey man , i 'm really not trying to edit war . it 's just that this guy is constantly removing relevant information and talking to me through edits instead of my talk page . he seems to care more about the formatting than the actual info .\"],\n",
       "       [\"`` more i ca n't make any real suggestion on improvement - i wondered if the section statistic should be later on , or a subsection of `` '' type of accident '' '' -i think the reference may need tidying so that they are all in the exact same format ie date format etc . i can do that later on , if no-one else doe first - if you have any preference for formatting style on reference or want to do it yourself please let me know . there appears to be a backlog on article for review so i guess there may be a delay until a reviewer turn up . it 's listed in the relevant form eg wikipedia : good_article_nominations # transport ``\"],\n",
       "       [\"you , sir , are my hero . any chance you remember what page that 's on ?\"]],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head().values#checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[\"explanation\\nwhy the edits made under my username hardcore metallica fan were reverted? they weren't vandalisms, just closure on some gas after i voted at new york dolls fac. and please don't remove the template from the talk page since i'm retired now.89.205.38.27\",\n",
       "        0],\n",
       "       [\"d'aww! he matches this background colour i'm seemingly stuck with. thanks.  (talk) 21:51, january 11, 2016 (utc)\",\n",
       "        0],\n",
       "       [\"hey man, i'm really not trying to edit war. it's just that this guy is constantly removing relevant information and talking to me through edits instead of my talk page. he seems to care more about the formatting than the actual info.\",\n",
       "        0],\n",
       "       ['\"\\nmore\\ni can\\'t make any real suggestions on improvement - i wondered if the section statistics should be later on, or a subsection of \"\"types of accidents\"\"  -i think the references may need tidying so that they are all in the exact same format ie date format etc. i can do that later on, if no-one else does first - if you have any preferences for formatting style on references or want to do it yourself please let me know.\\n\\nthere appears to be a backlog on articles for review so i guess there may be a delay until a reviewer turns up. it\\'s listed in the relevant form eg wikipedia:good_article_nominations#transport  \"',\n",
       "        0],\n",
       "       [\"you, sir, are my hero. any chance you remember what page that's on?\",\n",
       "        0]], dtype=object)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head().values#compare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I noticed the presence of the date / time when the comments were written, for our purposes this is unnecessary information, therefore I delete it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_date = []#function for line-by-line removal of redundant information\n",
    "for i in df['text']:\n",
    "    d = re.sub(r'january|utc|february|march|april|may|june|july|august|september|october|november|december','',i)\n",
    "    new_date.append(d)\n",
    "df = pd.DataFrame(new_date, columns = ['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove stop words from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = set(nltk.corpus.stopwords.words('english'))\n",
    "df['text'] = df['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))#function to remove stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[\"explanation edits made username hardcore metallica fan reverted ? n't vandalism , closure gas voted new york doll fac . please n't remove template talk page since 'm retired now.89.205.38.27\"],\n",
       "       [\"d'aww ! match background colour 'm seemingly stuck . thanks . ( talk ) 21:51 , 11 , 2016 ( )\"],\n",
       "       [\"hey man , 'm really trying edit war . 's guy constantly removing relevant information talking edits instead talk page . seems care formatting actual info .\"],\n",
       "       ...,\n",
       "       ['spitzer umm , actual article prostitution ring . - crunch captain .'],\n",
       "       ['look like wa actually put speedy first version deleted look .'],\n",
       "       [\"`` ... really n't think understand . came idea wa bad right away . kind community go `` '' bad idea '' '' go away , instead helping rewrite them. ``\"]],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.values#checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_date = []#I clean the dataset from unnecessary characters and spaces\n",
    "for i in df['text']:\n",
    "    d = [\" \".join(re.sub(r\"[^a-z!? ]\", ' ', str(i)).split())]\n",
    "    new_date.append(d)\n",
    "df = pd.DataFrame(new_date, columns = ['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['explanation edits made username hardcore metallica fan reverted ? n t vandalism closure gas voted new york doll fac please n t remove template talk page since m retired now'],\n",
       "       ['d aww ! match background colour m seemingly stuck thanks talk'],\n",
       "       ['hey man m really trying edit war s guy constantly removing relevant information talking edits instead talk page seems care formatting actual info'],\n",
       "       ['ca n t make real suggestion improvement wondered section statistic later subsection type accident i think reference need tidying exact format ie date format etc later no one else doe first preference formatting style reference want please let know appears backlog article review guess delay reviewer turn s listed relevant form eg wikipedia good article nominations transport'],\n",
       "       ['sir hero chance remember page s ?'],\n",
       "       ['congratulation well use tool well talk'],\n",
       "       ['cocksucker piss around work'],\n",
       "       ['vandalism matt shirvington article ha reverted please n t banned'],\n",
       "       ['sorry word nonsense wa offensive anyway m intending write anything article wow would jump vandalism m merely requesting encyclopedic one use school reference selective breeding page s almost stub point animal breeding short messy article give info must someone around expertise eugenics ?'],\n",
       "       ['alignment subject contrary dulithgow'],\n",
       "       ['fair use rationale image wonju jpg thanks uploading image wonju jpg notice image page specifies image used fair use explanation rationale use wikipedia article constitutes fair use addition boilerplate fair use template must also write image description page specific explanation rationale using image article consistent fair use please go image description page edit include fair use rationale uploaded fair use medium consider checking specified fair use rationale page find list image page edited clicking contribution link located top wikipedia page logged selecting image dropdown box note fair use image uploaded lacking explanation deleted one week uploaded described criterion speedy deletion question please ask medium copyright question page thank talk contribs unspecified source image wonju jpg thanks uploading image wonju jpg noticed file s description page currently doe n t specify created content copyright status unclear create file need specify owner copyright obtained website link website wa taken together restatement website s term use content usually sufficient information however copyright holder different website s publisher copyright also acknowledged well adding source please add proper copyright licensing tag file doe n t one already created took picture audio video tag used release gfdl believe medium meet criterion wikipedia fair use use tag one tag listed wikipedia image copyright tag fair use see wikipedia image copyright tag full list copyright tag use uploaded file consider checking specified source tagged find list file uploaded following link unsourced untagged image deleted one week tagged described criterion speedy deletion image copyrighted non free license per wikipedia fair use image deleted hour question please ask medium copyright question page thank talk contribs']],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(11).values#checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['editing article without consensus removal cited content without discussion find telling neither courtesy respond proposal k went ahead begin editing article without consensus reached let record note compelled reciprocate action make suggested change jj obviously lack serious knowledge area wa circumventing rr s point answering recent underinformed question ll focus kautilya significantly acquainted topic would understand kakatiyas feudatory eastern chalukya dynasty so question formal royal dynasty began natural question patent nonsense probably one reason previous editor changed old date uncertain wa date given first royal inscription based permanent grant hanumakonda original capital edification fief kakatiyas also suggest refresh knowledge wp civility improve quality current future discussion n t know author s hint ask politely rather first disrespectfully dismissing author demanding c v s book speaks history culture andhra pradesh old state included new state telangana arcane ivory tower speculative paper wp r want change date ce perfectly fine difference sense responsibility provide citation suggest let repeat perfectly fine respectful dialogue ve said take two hand clap ball court gentleman'],\n",
       "       ['read would thought everyone could recite heart say truly believe arbcom thankful jimbo wa ever taken arbcom read evidence voting thank valid question kelly always implied wa friend jimbo s good must pleasant however ca n t imagine jimbo wa particularly impressed blog either actually read blog wa pasted wiki lot information view area wa also commenting n t imagine jimbo much impressed many others concerned whole rotten case either s bit late concern actually bishonen s page view posted page decide post heartily sick whole case pity reference blog wa ever permitted evidence especially people fact want discus concerned party view it'],\n",
       "       ['auto guide motoring press good source encyclopedia article luxury vehicle automaker feed press lot b boastful superlative work hard write article adding even b describe product press earns credit hand feed add fact potential actual purchaser luxury vehicle insatiable appetite type b classifying particular vehicle luxury also designed easily make superior vehicle even difference superficial using self serving b marketing claim old invention help sell product well fuel fan base bragging right vehicle luxury hand difficult marketer humble genuine encyclopedia article ha avoid falling trap using traditional marketing superlative hyperbole'],\n",
       "       ['please identify part blp applies blp clearly state must follow npov say must present content relation manner subject covered reliable academic source case ha substantial mainstream academic source provided critical view claim chopra make trpod aka red pen doom'],\n",
       "       ['catalan independentism social movement involving people catalonia vote referendum barcelona one received political party leading council psc ciu vote total census voting favour referendum forbidden spanish court ha legal effect highly relevant'],\n",
       "       ['number parenthesis additional decimal point measured lie outside accuracy measurement process mean value parenthesis eg could used purpose calculation value result calculation would accurate number significant figure parenthesis considered word value parenthesis help avoid rounding error trying work limit accuracy gas constant agree commonly known confuses issue would recommend removing'],\n",
       "       ['second time asking view completely contradicts coverage reliable source anyone care feel ? ca n t even give consistent argument opening supposed mention significant aspect significant one ?'],\n",
       "       ['ashamed horrible thing put talk page'],\n",
       "       ['spitzer umm actual article prostitution ring crunch captain'],\n",
       "       ['look like wa actually put speedy first version deleted look'],\n",
       "       ['really n t think understand came idea wa bad right away kind community go bad idea go away instead helping rewrite them']],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail(11).values#compare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are words from one letter in the text, I consider them a little informative, although a part is not quite informative, but it turned into just n without a trace, nothing can be done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the metric on the logistic regression did not reach 0.75, I decided to keep short words, most of all, a piece of negation and question marks and exclamations carry meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the data into samples\n",
    "df['toxic'] = data['toxic']\n",
    "train, test = train_test_split(df, test_size=0.25, random_state=42)\n",
    "features_train = train['text']\n",
    "target_train = train['toxic']\n",
    "features_test = test['text']\n",
    "target_test = test['toxic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Процент класса \"1\" обучающей выборки: 10.165611056334498\n"
     ]
    }
   ],
   "source": [
    "print('Percentage of class \"1\" of the training sample:', len(target_train.loc[target_train==1])/len(target_train)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Процент класса \"1\" тестовой выборки: 10.174717368961973\n"
     ]
    }
   ],
   "source": [
    "print('Percentage of class \"1\" of the test sample:', len(target_test.loc[target_test==1])/len(target_test)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The percentages of classes remained roughly the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below cannot be loaded as a dataframe or an array, since such an array weighs 116 gigabytes, and training will take a long time, so I will resort to simple tokenization and training of neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = set(stopwords.words('english'))\n",
    "count_tf_idf = TfidfVectorizer(stop_words=stopwords) \n",
    "tf_idf = count_tf_idf.fit_transform(features_train)\n",
    "\n",
    "\n",
    "count_tf_idf_ts = TfidfVectorizer(stop_words=stopwords) \n",
    "tf_id_ts = count_tf_idf.transform(features_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since it didn’t work to convert words to tf-idf, I tokenize them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=20000)#tokinesis words with max word count = 20000\n",
    "tokenizer.fit_on_texts(features_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'article': 1,\n",
       " 's': 2,\n",
       " 't': 3,\n",
       " 'n': 4,\n",
       " 'page': 5,\n",
       " 'wa': 6,\n",
       " 'wikipedia': 7,\n",
       " 'talk': 8,\n",
       " 'ha': 9,\n",
       " 'one': 10,\n",
       " 'would': 11,\n",
       " 'please': 12,\n",
       " 'like': 13,\n",
       " 'see': 14,\n",
       " 'source': 15,\n",
       " 'also': 16,\n",
       " 'think': 17,\n",
       " 'know': 18,\n",
       " 'm': 19,\n",
       " 'time': 20,\n",
       " 'people': 21,\n",
       " 'user': 22,\n",
       " 'doe': 23,\n",
       " 'edit': 24,\n",
       " 'use': 25,\n",
       " 'make': 26,\n",
       " 'get': 27,\n",
       " 'image': 28,\n",
       " 'say': 29,\n",
       " 'thanks': 30,\n",
       " 'need': 31,\n",
       " 'name': 32,\n",
       " 'even': 33,\n",
       " 'link': 34,\n",
       " 'could': 35,\n",
       " 'good': 36,\n",
       " 'want': 37,\n",
       " 'well': 38,\n",
       " 're': 39,\n",
       " 'way': 40,\n",
       " 'information': 41,\n",
       " 've': 42,\n",
       " 'deletion': 43,\n",
       " 'comment': 44,\n",
       " 'editor': 45,\n",
       " 'go': 46,\n",
       " 'section': 47,\n",
       " 'u': 48,\n",
       " 'question': 49,\n",
       " 'help': 50,\n",
       " 'first': 51,\n",
       " 'thing': 52,\n",
       " 'wp': 53,\n",
       " 'fact': 54,\n",
       " 'new': 55,\n",
       " 'work': 56,\n",
       " 'look': 57,\n",
       " 'point': 58,\n",
       " 'editing': 59,\n",
       " 'edits': 60,\n",
       " 'discussion': 61,\n",
       " 'right': 62,\n",
       " 'thank': 63,\n",
       " 'made': 64,\n",
       " 'fuck': 65,\n",
       " 'many': 66,\n",
       " 'much': 67,\n",
       " 'used': 68,\n",
       " 'really': 69,\n",
       " 'find': 70,\n",
       " 'take': 71,\n",
       " 'deleted': 72,\n",
       " 'reference': 73,\n",
       " 'since': 74,\n",
       " 'read': 75,\n",
       " 'add': 76,\n",
       " 'list': 77,\n",
       " 'change': 78,\n",
       " 'someone': 79,\n",
       " 'reason': 80,\n",
       " 'wiki': 81,\n",
       " 'two': 82,\n",
       " 'back': 83,\n",
       " 'block': 84,\n",
       " 'policy': 85,\n",
       " 'year': 86,\n",
       " 'still': 87,\n",
       " 'll': 88,\n",
       " 'hi': 89,\n",
       " 'content': 90,\n",
       " 'state': 91,\n",
       " 'issue': 92,\n",
       " 'said': 93,\n",
       " 'case': 94,\n",
       " 'http': 95,\n",
       " 'word': 96,\n",
       " 'something': 97,\n",
       " 'mean': 98,\n",
       " 'going': 99,\n",
       " 'blocked': 100,\n",
       " 'd': 101,\n",
       " 'note': 102,\n",
       " 'stop': 103,\n",
       " 'place': 104,\n",
       " 'history': 105,\n",
       " 'without': 106,\n",
       " 'problem': 107,\n",
       " 'person': 108,\n",
       " 'added': 109,\n",
       " 'day': 110,\n",
       " 'tag': 111,\n",
       " 'another': 112,\n",
       " 'let': 113,\n",
       " 'might': 114,\n",
       " 'removed': 115,\n",
       " 'welcome': 116,\n",
       " 'part': 117,\n",
       " 'put': 118,\n",
       " 'sure': 119,\n",
       " 'subject': 120,\n",
       " 'however': 121,\n",
       " 'never': 122,\n",
       " 'done': 123,\n",
       " 'free': 124,\n",
       " 'vandalism': 125,\n",
       " 'feel': 126,\n",
       " 'personal': 127,\n",
       " 'using': 128,\n",
       " 'better': 129,\n",
       " 'come': 130,\n",
       " 'seems': 131,\n",
       " 'ask': 132,\n",
       " 'best': 133,\n",
       " 'actually': 134,\n",
       " 'show': 135,\n",
       " 'anything': 136,\n",
       " 'book': 137,\n",
       " 'keep': 138,\n",
       " 'claim': 139,\n",
       " 'view': 140,\n",
       " 'believe': 141,\n",
       " 'hope': 142,\n",
       " 'give': 143,\n",
       " 'site': 144,\n",
       " 'suck': 145,\n",
       " 'ca': 146,\n",
       " 'opinion': 147,\n",
       " 'copyright': 148,\n",
       " 'term': 149,\n",
       " 'already': 150,\n",
       " 'com': 151,\n",
       " 'remove': 152,\n",
       " 'attack': 153,\n",
       " 'nothing': 154,\n",
       " 'world': 155,\n",
       " 'post': 156,\n",
       " 'wrong': 157,\n",
       " 'though': 158,\n",
       " 'example': 159,\n",
       " 'little': 160,\n",
       " 'message': 161,\n",
       " 'must': 162,\n",
       " 'understand': 163,\n",
       " 'trying': 164,\n",
       " 'speedy': 165,\n",
       " 'others': 166,\n",
       " 'rule': 167,\n",
       " 'text': 168,\n",
       " 'long': 169,\n",
       " 'anyone': 170,\n",
       " 'last': 171,\n",
       " 'account': 172,\n",
       " 'request': 173,\n",
       " 'e': 174,\n",
       " 'agree': 175,\n",
       " 'fair': 176,\n",
       " 'war': 177,\n",
       " 'try': 178,\n",
       " 'contribution': 179,\n",
       " 'number': 180,\n",
       " 'life': 181,\n",
       " 'style': 182,\n",
       " 'template': 183,\n",
       " 'ip': 184,\n",
       " 'different': 185,\n",
       " 'rather': 186,\n",
       " 'reliable': 187,\n",
       " 'english': 188,\n",
       " 'sorry': 189,\n",
       " 'making': 190,\n",
       " 'thought': 191,\n",
       " 'continue': 192,\n",
       " 'got': 193,\n",
       " 'found': 194,\n",
       " 'great': 195,\n",
       " 'matter': 196,\n",
       " 'non': 197,\n",
       " 'shit': 198,\n",
       " 'language': 199,\n",
       " 'leave': 200,\n",
       " 'lot': 201,\n",
       " 'title': 202,\n",
       " 'original': 203,\n",
       " 'check': 204,\n",
       " 'else': 205,\n",
       " 'statement': 206,\n",
       " 'website': 207,\n",
       " 'c': 208,\n",
       " 'simply': 209,\n",
       " 'support': 210,\n",
       " 'top': 211,\n",
       " 'adding': 212,\n",
       " 'group': 213,\n",
       " 'every': 214,\n",
       " 'probably': 215,\n",
       " 'guideline': 216,\n",
       " 'material': 217,\n",
       " 'consensus': 218,\n",
       " 'created': 219,\n",
       " 'hello': 220,\n",
       " 'review': 221,\n",
       " 'either': 222,\n",
       " 'delete': 223,\n",
       " 'etc': 224,\n",
       " 'least': 225,\n",
       " 'admin': 226,\n",
       " 'version': 227,\n",
       " 'enough': 228,\n",
       " 'notable': 229,\n",
       " 'criterion': 230,\n",
       " 'called': 231,\n",
       " 'idea': 232,\n",
       " 'yes': 233,\n",
       " 'tell': 234,\n",
       " 'far': 235,\n",
       " 'bad': 236,\n",
       " 'yet': 237,\n",
       " 'around': 238,\n",
       " 'write': 239,\n",
       " 'mention': 240,\n",
       " 'www': 241,\n",
       " 'reverted': 242,\n",
       " 'real': 243,\n",
       " 'old': 244,\n",
       " 'given': 245,\n",
       " 'encyclopedia': 246,\n",
       " 'category': 247,\n",
       " 'topic': 248,\n",
       " 'evidence': 249,\n",
       " 'jew': 250,\n",
       " 'date': 251,\n",
       " 'clearly': 252,\n",
       " 'bit': 253,\n",
       " 'revert': 254,\n",
       " 'seem': 255,\n",
       " 'ever': 256,\n",
       " 'guy': 257,\n",
       " 'picture': 258,\n",
       " 'american': 259,\n",
       " 'country': 260,\n",
       " 'clear': 261,\n",
       " 'org': 262,\n",
       " 'notice': 263,\n",
       " 'saying': 264,\n",
       " 'instead': 265,\n",
       " 'important': 266,\n",
       " 'correct': 267,\n",
       " 'fucking': 268,\n",
       " 'medium': 269,\n",
       " 'pov': 270,\n",
       " 'always': 271,\n",
       " 'i': 272,\n",
       " 'written': 273,\n",
       " 'true': 274,\n",
       " 'address': 275,\n",
       " 'quite': 276,\n",
       " 'whether': 277,\n",
       " 'research': 278,\n",
       " 'based': 279,\n",
       " 'perhaps': 280,\n",
       " 'administrator': 281,\n",
       " 'warning': 282,\n",
       " 'consider': 283,\n",
       " 'gay': 284,\n",
       " 'a': 285,\n",
       " 'getting': 286,\n",
       " 'citation': 287,\n",
       " 'three': 288,\n",
       " 'second': 289,\n",
       " 'school': 290,\n",
       " 'sentence': 291,\n",
       " 'call': 292,\n",
       " 'start': 293,\n",
       " 'line': 294,\n",
       " 'considered': 295,\n",
       " 'current': 296,\n",
       " 'hey': 297,\n",
       " 'several': 298,\n",
       " 'kind': 299,\n",
       " 'argument': 300,\n",
       " 'file': 301,\n",
       " 'action': 302,\n",
       " 'end': 303,\n",
       " 'summary': 304,\n",
       " 'general': 305,\n",
       " 'following': 306,\n",
       " 'hate': 307,\n",
       " 'man': 308,\n",
       " 'left': 309,\n",
       " 'b': 310,\n",
       " 'course': 311,\n",
       " 'game': 312,\n",
       " 'oh': 313,\n",
       " 'project': 314,\n",
       " 'p': 315,\n",
       " 'possible': 316,\n",
       " 'listed': 317,\n",
       " 'lol': 318,\n",
       " 'move': 319,\n",
       " 'care': 320,\n",
       " 'le': 321,\n",
       " 'main': 322,\n",
       " 'regarding': 323,\n",
       " 'common': 324,\n",
       " 'quote': 325,\n",
       " 'die': 326,\n",
       " 'include': 327,\n",
       " 'interest': 328,\n",
       " 'whole': 329,\n",
       " 'mentioned': 330,\n",
       " 'wish': 331,\n",
       " 'report': 332,\n",
       " 'f': 333,\n",
       " 'known': 334,\n",
       " 'love': 335,\n",
       " 'en': 336,\n",
       " 'seen': 337,\n",
       " 'create': 338,\n",
       " 'community': 339,\n",
       " 'mind': 340,\n",
       " 'jpg': 341,\n",
       " 'answer': 342,\n",
       " 'sense': 343,\n",
       " 'big': 344,\n",
       " 'related': 345,\n",
       " 'member': 346,\n",
       " 'party': 347,\n",
       " 'contribs': 348,\n",
       " 'week': 349,\n",
       " 'fat': 350,\n",
       " 'including': 351,\n",
       " 'th': 352,\n",
       " 'entry': 353,\n",
       " 'company': 354,\n",
       " 'ok': 355,\n",
       " 'notability': 356,\n",
       " 'sign': 357,\n",
       " 'info': 358,\n",
       " 'specific': 359,\n",
       " 'r': 360,\n",
       " 'provide': 361,\n",
       " 'result': 362,\n",
       " 'neutral': 363,\n",
       " 'redirect': 364,\n",
       " 'position': 365,\n",
       " 'news': 366,\n",
       " 'order': 367,\n",
       " 'suggest': 368,\n",
       " 'four': 369,\n",
       " 'city': 370,\n",
       " 'happy': 371,\n",
       " 'g': 372,\n",
       " 'removing': 373,\n",
       " 'standard': 374,\n",
       " 'changed': 375,\n",
       " 'power': 376,\n",
       " 'nigger': 377,\n",
       " 'included': 378,\n",
       " 'started': 379,\n",
       " 'type': 380,\n",
       " 'writing': 381,\n",
       " 'next': 382,\n",
       " 'test': 383,\n",
       " 'appropriate': 384,\n",
       " 'explain': 385,\n",
       " 'color': 386,\n",
       " 'system': 387,\n",
       " 'single': 388,\n",
       " 'lead': 389,\n",
       " 'regard': 390,\n",
       " 'discus': 391,\n",
       " 'recent': 392,\n",
       " 'although': 393,\n",
       " 'anyway': 394,\n",
       " 'per': 395,\n",
       " 'side': 396,\n",
       " 'god': 397,\n",
       " 'faith': 398,\n",
       " 'law': 399,\n",
       " 'looking': 400,\n",
       " 'full': 401,\n",
       " 'wrote': 402,\n",
       " 'dispute': 403,\n",
       " 'paragraph': 404,\n",
       " 'relevant': 405,\n",
       " 'dont': 406,\n",
       " 'official': 407,\n",
       " 'public': 408,\n",
       " 'band': 409,\n",
       " 'film': 410,\n",
       " 'form': 411,\n",
       " 'process': 412,\n",
       " 'especially': 413,\n",
       " 'away': 414,\n",
       " 'able': 415,\n",
       " 'response': 416,\n",
       " 'theory': 417,\n",
       " 'later': 418,\n",
       " 'currently': 419,\n",
       " 'area': 420,\n",
       " 'taken': 421,\n",
       " 'stuff': 422,\n",
       " 'self': 423,\n",
       " 'month': 424,\n",
       " 'friend': 425,\n",
       " 'high': 426,\n",
       " 'nice': 427,\n",
       " 'everyone': 428,\n",
       " 'interested': 429,\n",
       " 'anti': 430,\n",
       " 'within': 431,\n",
       " 'author': 432,\n",
       " 'certainly': 433,\n",
       " 'addition': 434,\n",
       " 'unless': 435,\n",
       " 'cunt': 436,\n",
       " 'appears': 437,\n",
       " 'wanted': 438,\n",
       " 'web': 439,\n",
       " 'detail': 440,\n",
       " 'background': 441,\n",
       " 'truth': 442,\n",
       " 'stay': 443,\n",
       " 'today': 444,\n",
       " 'everything': 445,\n",
       " 'live': 446,\n",
       " 'reader': 447,\n",
       " 'reading': 448,\n",
       " 'edited': 449,\n",
       " 'according': 450,\n",
       " 'pretty': 451,\n",
       " 'completely': 452,\n",
       " 'moron': 453,\n",
       " 'hard': 454,\n",
       " 'admins': 455,\n",
       " 'published': 456,\n",
       " 'remember': 457,\n",
       " 'due': 458,\n",
       " 'government': 459,\n",
       " 'wo': 460,\n",
       " 'sandbox': 461,\n",
       " 'story': 462,\n",
       " 'event': 463,\n",
       " 'came': 464,\n",
       " 'obviously': 465,\n",
       " 'therefore': 466,\n",
       " 'future': 467,\n",
       " 'involved': 468,\n",
       " 'piece': 469,\n",
       " 'hour': 470,\n",
       " 'reply': 471,\n",
       " 'asked': 472,\n",
       " 'cite': 473,\n",
       " 'national': 474,\n",
       " 'hand': 475,\n",
       " 'posted': 476,\n",
       " 'definition': 477,\n",
       " 'explanation': 478,\n",
       " 'past': 479,\n",
       " 'character': 480,\n",
       " 'sound': 481,\n",
       " 'learn': 482,\n",
       " 'photo': 483,\n",
       " 'placed': 484,\n",
       " 'talking': 485,\n",
       " 'search': 486,\n",
       " 'sort': 487,\n",
       " 'ago': 488,\n",
       " 'attempt': 489,\n",
       " 'similar': 490,\n",
       " 'working': 491,\n",
       " 'political': 492,\n",
       " 'took': 493,\n",
       " 'present': 494,\n",
       " 'whatever': 495,\n",
       " 'small': 496,\n",
       " 'exactly': 497,\n",
       " 'google': 498,\n",
       " 'false': 499,\n",
       " 'useful': 500,\n",
       " 'white': 501,\n",
       " 'ban': 502,\n",
       " 'description': 503,\n",
       " 'record': 504,\n",
       " 'meet': 505,\n",
       " 'five': 506,\n",
       " 'british': 507,\n",
       " 'deleting': 508,\n",
       " 'particular': 509,\n",
       " 'united': 510,\n",
       " 'class': 511,\n",
       " 'error': 512,\n",
       " 'faggot': 513,\n",
       " 'noticed': 514,\n",
       " 'dick': 515,\n",
       " 'concern': 516,\n",
       " 'family': 517,\n",
       " 'v': 518,\n",
       " 'appreciate': 519,\n",
       " 'child': 520,\n",
       " 'set': 521,\n",
       " 'major': 522,\n",
       " 'wikiproject': 523,\n",
       " 'pig': 524,\n",
       " 'often': 525,\n",
       " 'university': 526,\n",
       " 'guess': 527,\n",
       " 'needed': 528,\n",
       " 'violation': 529,\n",
       " 'copy': 530,\n",
       " 'provided': 531,\n",
       " 'criticism': 532,\n",
       " 'stated': 533,\n",
       " 'reverting': 534,\n",
       " 'npov': 535,\n",
       " 'almost': 536,\n",
       " 'status': 537,\n",
       " 'become': 538,\n",
       " 'act': 539,\n",
       " 'open': 540,\n",
       " 'stupid': 541,\n",
       " 'german': 542,\n",
       " 'conflict': 543,\n",
       " 'black': 544,\n",
       " 'death': 545,\n",
       " 'tried': 546,\n",
       " 'along': 547,\n",
       " 'human': 548,\n",
       " 'banned': 549,\n",
       " 'vandalize': 550,\n",
       " 'video': 551,\n",
       " 'email': 552,\n",
       " 'woman': 553,\n",
       " 'taking': 554,\n",
       " 'fine': 555,\n",
       " 'job': 556,\n",
       " 'username': 557,\n",
       " 'music': 558,\n",
       " 'entire': 559,\n",
       " 'cheer': 560,\n",
       " 'certain': 561,\n",
       " 'knowledge': 562,\n",
       " 'generally': 563,\n",
       " 'otherwise': 564,\n",
       " 'bitch': 565,\n",
       " 'alone': 566,\n",
       " 'soon': 567,\n",
       " 'short': 568,\n",
       " 'the': 569,\n",
       " 'archive': 570,\n",
       " 'follow': 571,\n",
       " 'vote': 572,\n",
       " 'cause': 573,\n",
       " 'cited': 574,\n",
       " 'aware': 575,\n",
       " 'recently': 576,\n",
       " 'difference': 577,\n",
       " 'figure': 578,\n",
       " 'level': 579,\n",
       " 'contact': 580,\n",
       " 'unblock': 581,\n",
       " 'mistake': 582,\n",
       " 'mr': 583,\n",
       " 'john': 584,\n",
       " 'uploaded': 585,\n",
       " 'science': 586,\n",
       " 'sex': 587,\n",
       " 'indeed': 588,\n",
       " 'context': 589,\n",
       " 'likely': 590,\n",
       " 'internet': 591,\n",
       " 'appear': 592,\n",
       " 'saw': 593,\n",
       " 'interesting': 594,\n",
       " 'deal': 595,\n",
       " 'meaning': 596,\n",
       " 'actual': 597,\n",
       " 'simple': 598,\n",
       " 'ball': 599,\n",
       " 'bias': 600,\n",
       " 'told': 601,\n",
       " 'contributing': 602,\n",
       " 'improve': 603,\n",
       " 'jewish': 604,\n",
       " 'x': 605,\n",
       " 'proposed': 606,\n",
       " 'obvious': 607,\n",
       " 'sourced': 608,\n",
       " 'de': 609,\n",
       " 'biography': 610,\n",
       " 'individual': 611,\n",
       " 'uk': 612,\n",
       " 'force': 613,\n",
       " 'us': 614,\n",
       " 'study': 615,\n",
       " 'decide': 616,\n",
       " 'play': 617,\n",
       " 'song': 618,\n",
       " 'suggestion': 619,\n",
       " 'external': 620,\n",
       " 'third': 621,\n",
       " 'purpose': 622,\n",
       " 'allowed': 623,\n",
       " 'attention': 624,\n",
       " 'million': 625,\n",
       " 'proof': 626,\n",
       " 'head': 627,\n",
       " 'thus': 628,\n",
       " 'went': 629,\n",
       " 'and': 630,\n",
       " 'moved': 631,\n",
       " 'disagree': 632,\n",
       " 'various': 633,\n",
       " 'previous': 634,\n",
       " 'christian': 635,\n",
       " 'organization': 636,\n",
       " 'fish': 637,\n",
       " 'nonsense': 638,\n",
       " 'effort': 639,\n",
       " 'debate': 640,\n",
       " 'run': 641,\n",
       " 'you': 642,\n",
       " 'enjoy': 643,\n",
       " 'im': 644,\n",
       " 'w': 645,\n",
       " 'stand': 646,\n",
       " 'freedom': 647,\n",
       " 'dog': 648,\n",
       " 'team': 649,\n",
       " 'sock': 650,\n",
       " 'table': 651,\n",
       " 'church': 652,\n",
       " 'creating': 653,\n",
       " 'situation': 654,\n",
       " 'respect': 655,\n",
       " 'lie': 656,\n",
       " 'avoid': 657,\n",
       " 'greek': 658,\n",
       " 'large': 659,\n",
       " 'happened': 660,\n",
       " 'biased': 661,\n",
       " 'lack': 662,\n",
       " 'doubt': 663,\n",
       " 'worked': 664,\n",
       " 'together': 665,\n",
       " 'longer': 666,\n",
       " 'seriously': 667,\n",
       " 'accepted': 668,\n",
       " 'cover': 669,\n",
       " 'automatically': 670,\n",
       " 'proper': 671,\n",
       " 'helpful': 672,\n",
       " 'series': 673,\n",
       " 'in': 674,\n",
       " 'available': 675,\n",
       " 'valid': 676,\n",
       " 'manual': 677,\n",
       " 'afd': 678,\n",
       " 'tilde': 679,\n",
       " 'contributor': 680,\n",
       " 'album': 681,\n",
       " 'exist': 682,\n",
       " 'box': 683,\n",
       " 'space': 684,\n",
       " 'living': 685,\n",
       " 'cock': 686,\n",
       " 'historical': 687,\n",
       " 'computer': 688,\n",
       " 'fix': 689,\n",
       " 'fan': 690,\n",
       " 'width': 691,\n",
       " 'south': 692,\n",
       " 'field': 693,\n",
       " 'culture': 694,\n",
       " 'calling': 695,\n",
       " 'quality': 696,\n",
       " 'multiple': 697,\n",
       " 'indicate': 698,\n",
       " 'upon': 699,\n",
       " 'necessary': 700,\n",
       " 'rest': 701,\n",
       " 'india': 702,\n",
       " 'decision': 703,\n",
       " 'penis': 704,\n",
       " 'pro': 705,\n",
       " 'service': 706,\n",
       " 'usually': 707,\n",
       " 'eye': 708,\n",
       " 'rationale': 709,\n",
       " 'assume': 710,\n",
       " 'accept': 711,\n",
       " 'vandal': 712,\n",
       " 'hell': 713,\n",
       " 'accurate': 714,\n",
       " 'period': 715,\n",
       " 'blocking': 716,\n",
       " 'personally': 717,\n",
       " 'episode': 718,\n",
       " 'j': 719,\n",
       " 'release': 720,\n",
       " 'aid': 721,\n",
       " 'wanker': 722,\n",
       " 'yeah': 723,\n",
       " 'asshole': 724,\n",
       " 'refer': 725,\n",
       " 'explaining': 726,\n",
       " 'map': 727,\n",
       " 'separate': 728,\n",
       " 'fag': 729,\n",
       " 'nazi': 730,\n",
       " 'tagged': 731,\n",
       " 'online': 732,\n",
       " 'heard': 733,\n",
       " 'behavior': 734,\n",
       " 'serious': 735,\n",
       " 'belief': 736,\n",
       " 'bark': 737,\n",
       " 'house': 738,\n",
       " 'watch': 739,\n",
       " 'produce': 740,\n",
       " 'century': 741,\n",
       " 'special': 742,\n",
       " 'html': 743,\n",
       " 'apparently': 744,\n",
       " 'data': 745,\n",
       " 'complete': 746,\n",
       " 'none': 747,\n",
       " 'effect': 748,\n",
       " 'business': 749,\n",
       " 'speak': 750,\n",
       " 'movie': 751,\n",
       " 'k': 752,\n",
       " 'border': 753,\n",
       " 'abuse': 754,\n",
       " 'legal': 755,\n",
       " 'couple': 756,\n",
       " 'religion': 757,\n",
       " 'close': 758,\n",
       " 'king': 759,\n",
       " 'mother': 760,\n",
       " 'prove': 761,\n",
       " 'student': 762,\n",
       " 'changing': 763,\n",
       " 'military': 764,\n",
       " 'directly': 765,\n",
       " 'wikipedian': 766,\n",
       " 'red': 767,\n",
       " 'primary': 768,\n",
       " 'threat': 769,\n",
       " 'bot': 770,\n",
       " 'existing': 771,\n",
       " 'paper': 772,\n",
       " 'accusation': 773,\n",
       " 'home': 774,\n",
       " 'asking': 775,\n",
       " 'eat': 776,\n",
       " 'pillar': 777,\n",
       " 'tv': 778,\n",
       " 'modern': 779,\n",
       " 'control': 780,\n",
       " 'creation': 781,\n",
       " 'contribute': 782,\n",
       " 'disruptive': 783,\n",
       " 'supposed': 784,\n",
       " 'early': 785,\n",
       " 'court': 786,\n",
       " 'except': 787,\n",
       " 'meant': 788,\n",
       " 'warring': 789,\n",
       " 'among': 790,\n",
       " 'population': 791,\n",
       " 'club': 792,\n",
       " 'described': 793,\n",
       " 'chance': 794,\n",
       " 'age': 795,\n",
       " 'significant': 796,\n",
       " 'born': 797,\n",
       " 'award': 798,\n",
       " 'half': 799,\n",
       " 'experience': 800,\n",
       " 'specifically': 801,\n",
       " 'l': 802,\n",
       " 'scientific': 803,\n",
       " 'civil': 804,\n",
       " 'nomination': 805,\n",
       " 'muslim': 806,\n",
       " 'kill': 807,\n",
       " 'incorrect': 808,\n",
       " 'particularly': 809,\n",
       " 'israel': 810,\n",
       " 'america': 811,\n",
       " 'okay': 812,\n",
       " 'sometimes': 813,\n",
       " 'posting': 814,\n",
       " 'light': 815,\n",
       " 'inclusion': 816,\n",
       " 'press': 817,\n",
       " 'tutorial': 818,\n",
       " 'body': 819,\n",
       " 'contest': 820,\n",
       " 'allow': 821,\n",
       " 'idiot': 822,\n",
       " 'international': 823,\n",
       " 'license': 824,\n",
       " 'outside': 825,\n",
       " 'earlier': 826,\n",
       " 'na': 827,\n",
       " 'north': 828,\n",
       " 'st': 829,\n",
       " 'wait': 830,\n",
       " 'independent': 831,\n",
       " 'possibly': 832,\n",
       " 'coming': 833,\n",
       " 'reported': 834,\n",
       " 'finally': 835,\n",
       " 'gave': 836,\n",
       " 'worth': 837,\n",
       " 'nation': 838,\n",
       " 'linked': 839,\n",
       " 'giving': 840,\n",
       " 'removal': 841,\n",
       " 'men': 842,\n",
       " 'thinking': 843,\n",
       " 'anonymous': 844,\n",
       " 'happen': 845,\n",
       " 'requesting': 846,\n",
       " 'respond': 847,\n",
       " 'al': 848,\n",
       " 'decided': 849,\n",
       " 'movement': 850,\n",
       " 'co': 851,\n",
       " 'concept': 852,\n",
       " 'majority': 853,\n",
       " 'dear': 854,\n",
       " 'align': 855,\n",
       " 'nipple': 856,\n",
       " 'advice': 857,\n",
       " 'value': 858,\n",
       " 'final': 859,\n",
       " 'towards': 860,\n",
       " 'bring': 861,\n",
       " 'absolutely': 862,\n",
       " 'featured': 863,\n",
       " 'blog': 864,\n",
       " 'art': 865,\n",
       " 'discussed': 866,\n",
       " 'unsigned': 867,\n",
       " 'entirely': 868,\n",
       " 'putting': 869,\n",
       " 'mark': 870,\n",
       " 'merely': 871,\n",
       " 'authority': 872,\n",
       " 'russian': 873,\n",
       " 'step': 874,\n",
       " 'lost': 875,\n",
       " 'named': 876,\n",
       " 'totally': 877,\n",
       " 'looked': 878,\n",
       " 'amount': 879,\n",
       " 'college': 880,\n",
       " 'count': 881,\n",
       " 'super': 882,\n",
       " 'expert': 883,\n",
       " 'px': 884,\n",
       " 'face': 885,\n",
       " 'of': 886,\n",
       " 'product': 887,\n",
       " 'minor': 888,\n",
       " 'irrelevant': 889,\n",
       " 'hit': 890,\n",
       " 'dead': 891,\n",
       " 'infobox': 892,\n",
       " 'french': 893,\n",
       " 'poor': 894,\n",
       " 'mine': 895,\n",
       " 'preceding': 896,\n",
       " 'acceptable': 897,\n",
       " 'damn': 898,\n",
       " 'inappropriate': 899,\n",
       " 'hitler': 900,\n",
       " 'understanding': 901,\n",
       " 'log': 902,\n",
       " 'player': 903,\n",
       " 'hold': 904,\n",
       " 'nobody': 905,\n",
       " 'ga': 906,\n",
       " 'controversy': 907,\n",
       " 'board': 908,\n",
       " 'importance': 909,\n",
       " 'direct': 910,\n",
       " 'gone': 911,\n",
       " 'highly': 912,\n",
       " 'practice': 913,\n",
       " 'o': 914,\n",
       " 'stub': 915,\n",
       " 'academic': 916,\n",
       " 'sent': 917,\n",
       " 'neither': 918,\n",
       " 'cool': 919,\n",
       " 'religious': 920,\n",
       " 'indian': 921,\n",
       " 'spam': 922,\n",
       " 'controversial': 923,\n",
       " 'phrase': 924,\n",
       " 'barnstar': 925,\n",
       " 'behind': 926,\n",
       " 'experiment': 927,\n",
       " 'origin': 928,\n",
       " 'taco': 929,\n",
       " 'forum': 930,\n",
       " 'finding': 931,\n",
       " 'guide': 932,\n",
       " 'racist': 933,\n",
       " 'unfortunately': 934,\n",
       " 'basic': 935,\n",
       " 'bullshit': 936,\n",
       " 'region': 937,\n",
       " 'across': 938,\n",
       " 'joke': 939,\n",
       " 'vertical': 940,\n",
       " 'introduction': 941,\n",
       " 'battle': 942,\n",
       " 'it': 943,\n",
       " 'local': 944,\n",
       " 'verifiable': 945,\n",
       " 'minute': 946,\n",
       " 'document': 947,\n",
       " 'properly': 948,\n",
       " 'nature': 949,\n",
       " 'troll': 950,\n",
       " 'despite': 951,\n",
       " 'shown': 952,\n",
       " 'missing': 953,\n",
       " 'turn': 954,\n",
       " 'money': 955,\n",
       " 'army': 956,\n",
       " 'diff': 957,\n",
       " 'instance': 958,\n",
       " 'letter': 959,\n",
       " 'definitely': 960,\n",
       " 'update': 961,\n",
       " 'moment': 962,\n",
       " 'format': 963,\n",
       " 'european': 964,\n",
       " 'fun': 965,\n",
       " 'strong': 966,\n",
       " 'former': 967,\n",
       " 'easily': 968,\n",
       " 'code': 969,\n",
       " 'season': 970,\n",
       " 'required': 971,\n",
       " 'easy': 972,\n",
       " 'match': 973,\n",
       " 'david': 974,\n",
       " 'proposal': 975,\n",
       " 'boy': 976,\n",
       " 'rr': 977,\n",
       " 'wonder': 978,\n",
       " 'tool': 979,\n",
       " 'ahead': 980,\n",
       " 'race': 981,\n",
       " 'basis': 982,\n",
       " 'usage': 983,\n",
       " 'flag': 984,\n",
       " 'expect': 985,\n",
       " 'propaganda': 986,\n",
       " 'established': 987,\n",
       " 'newspaper': 988,\n",
       " 'role': 989,\n",
       " 'speedily': 990,\n",
       " 'president': 991,\n",
       " 'rfc': 992,\n",
       " 'police': 993,\n",
       " 'social': 994,\n",
       " 'forward': 995,\n",
       " 'access': 996,\n",
       " 'difficult': 997,\n",
       " 'wikipedians': 998,\n",
       " 'explained': 999,\n",
       " 'middle': 1000,\n",
       " ...}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_index#checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n t worry think fixed still look cramped\n",
      "[4, 3, 1314, 17, 1041, 87, 57]\n"
     ]
    }
   ],
   "source": [
    "#allocate tokens for verification\n",
    "sequences = tokenizer.texts_to_sequences(features_train)\n",
    "index = 1\n",
    "print(features_train[154769])#since the dataset was split in random order, I traced that index 1 in the data itself has index 154769\n",
    "print(sequences[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_index['think']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization was successful. Next, I'll save tokenized data with a 90-word limit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = pad_sequences(sequences, maxlen=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sequences = tokenizer.texts_to_sequences(features_test)\n",
    "x_test = pad_sequences(test_sequences, maxlen=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft = pd.get_dummies(target_train)#transform the answers for the output layer of the neural network = 2\n",
    "fts = pd.get_dummies(target_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion: the data was preprocessed: stop words, lemmatization were removed, reduced to lower case, uninformative words were removed. The data is ready to train the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will train neural networks. The first model will be built on a convolutional neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cnn = Sequential()\n",
    "model_cnn.add(Embedding(20000, 32))#number of words 20000\n",
    "model_cnn.add(Conv1D(250, 5, padding='valid', activation='relu'))\n",
    "model_cnn.add(GlobalMaxPooling1D())\n",
    "model_cnn.add(Dense(128, activation='relu'))\n",
    "model_cnn.add(Dense(2, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cnn.compile(optimizer='adam', \n",
    "              loss='binary_crossentropy',\n",
    "              metrics=[tfa.metrics.F1Score(average='macro',num_classes = 2)])#metric f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 32)          640000    \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, None, 250)         40250     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d (Global (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               32128     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 712,636\n",
      "Trainable params: 712,636\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 107710 samples, validate on 11968 samples\n",
      "107710/107710 [==============================] - 145s 1ms/sample - loss: 0.1948 - f1_score: 0.7583 - val_loss: 0.1150 - val_f1_score: 0.8709\n"
     ]
    }
   ],
   "source": [
    "history_cnn = model_cnn.fit(x_train, \n",
    "                            ft, \n",
    "                            epochs=1,\n",
    "                            batch_size=300,\n",
    "                            validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#I build a graph of the quality of training according to the selected metric\n",
    "plt.plot(history_cnn.history['f1_score'], \n",
    "         label='Proportion of correct answers on the training set')\n",
    "plt.plot(history_cnn.history['val_f1_score'], \n",
    "         label='Proportion of correct answers on the test set')\n",
    "plt.xlabel('Age of Learning')\n",
    "plt.ylabel('Percentage of correct answers')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I check the quality of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39893/39893 [==============================] - 16s 408us/sample - loss: 0.1136 - f1_score: 0.8734\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.11357230920748655, 0.8734446]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_cnn.evaluate(x_test, fts, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f1 is equal to 0.8734."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am training an LSTM network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lstm = Sequential()\n",
    "model_lstm.add(Embedding(20000, 32, input_length=90))\n",
    "model_lstm.add(LSTM(16))\n",
    "model_lstm.add(Dense(2, activation='softmax'))\n",
    "model_lstm.compile(optimizer='adam', \n",
    "              loss='binary_crossentropy', \n",
    "              metrics=[tfa.metrics.F1Score(average='macro',num_classes = 2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 90, 32)            640000    \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 16)                3136      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 34        \n",
      "=================================================================\n",
      "Total params: 643,170\n",
      "Trainable params: 643,170\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_lstm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 107710 samples, validate on 11968 samples\n",
      "Epoch 1/2\n",
      "107710/107710 [==============================] - 97s 898us/sample - loss: 0.2250 - f1_score: 0.7091 - val_loss: 0.1351 - val_f1_score: 0.8504\n",
      "Epoch 2/2\n",
      "107710/107710 [==============================] - 88s 821us/sample - loss: 0.1083 - f1_score: 0.8856 - val_loss: 0.1172 - val_f1_score: 0.8747\n"
     ]
    }
   ],
   "source": [
    "history_lstm = model_lstm.fit(x_train, \n",
    "                              ft, \n",
    "                              epochs=2,\n",
    "                              batch_size=300,\n",
    "                              validation_split=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking f1 of LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39893/39893 [==============================] - 32s 798us/sample - loss: 0.1192 - f1_score: 0.8775\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.11922623122491167, 0.8774971]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_lstm.evaluate(x_test, fts, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will check the answers of the convolutional neural network model on one column of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "tess = pd.DataFrame(model_cnn.predict(x_test),columns=['one','two'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.5915031e-01, 4.0849626e-02],\n",
       "       [9.9926382e-01, 7.3618605e-04],\n",
       "       [9.3792099e-01, 6.2078945e-02],\n",
       "       ...,\n",
       "       [9.9913579e-01, 8.6417695e-04],\n",
       "       [9.8690933e-01, 1.3090711e-02],\n",
       "       [9.2599833e-01, 7.4001625e-02]], dtype=float32)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_cnn.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the model does not specifically output 1 or 0, let's round the values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "tess['one'] = tess['one'].round()\n",
    "tess['two'] = tess['two'].round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>one</th>\n",
       "      <th>two</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39888</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39889</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39890</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39891</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39892</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>39893 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       one  two\n",
       "0      1.0  0.0\n",
       "1      1.0  0.0\n",
       "2      1.0  0.0\n",
       "3      1.0  0.0\n",
       "4      1.0  0.0\n",
       "...    ...  ...\n",
       "39888  1.0  0.0\n",
       "39889  0.0  1.0\n",
       "39890  1.0  0.0\n",
       "39891  1.0  0.0\n",
       "39892  1.0  0.0\n",
       "\n",
       "[39893 rows x 2 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "119105    0\n",
       "131631    0\n",
       "125326    0\n",
       "111256    0\n",
       "83590     0\n",
       "         ..\n",
       "11066     0\n",
       "141125    1\n",
       "158236    0\n",
       "105667    0\n",
       "155646    0\n",
       "Name: toxic, Length: 39893, dtype: int64"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Реальное f1 сверточной нейронной сети: 0.7694256756756758\n"
     ]
    }
   ],
   "source": [
    "print('Real f1 convolutional neural network:',f1_score(target_test,tess['two']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainn, valid = train_test_split(train, test_size=0.1, random_state=42)\n",
    "features_train = train['text']\n",
    "target_train = train['toxic']\n",
    "features_valid = test['text']\n",
    "target_valid = test['toxic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegressionCV(random_state=42,class_weight='balanced',cv=5, max_iter =200,n_jobs=-1,\n",
    "                          scoring='f1').fit(count_tf_idf.transform(features_train), target_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 логистической регрессии на обучении: 0.9356332484371384\n",
      "f1 логистической регрессии на валидации: 0.7651241403426974\n",
      "f1 логистической регрессии на тесте: 0.7651241403426974\n"
     ]
    }
   ],
   "source": [
    "print('f1 logistic regression on training:',f1_score(clf.predict(tf_id_ts),target_train))\n",
    "print('f1 logistic regression on validation:',f1_score(clf.predict(count_tf_idf.transform(features_valid)),target_valid))\n",
    "print('f1 logistic regression on test:',f1_score(clf.predict(count_tf_idf.transform(features_test)),target_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f1 is equal to 0.7651."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The text data was preprocessed, 2 classification models were trained, which showed an f1 value of more than 0.86. 2, the epoch in the model was already selected in advance, since the epochs are not saved on the remote Yandex server."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
